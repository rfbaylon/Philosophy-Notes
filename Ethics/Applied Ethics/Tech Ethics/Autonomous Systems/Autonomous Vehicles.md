#### Fun Resources
[The Moral Machine](https://www.moralmachine.net/)
[Absurd Trolley Problems](https://neal.fun/absurd-trolley-problems/)
### Ethical Issues
1. Autonomous vehicles
2. Accident scenarios
3. Job loss (Especially in trucking)
3. R&D and Beta testing
4. Following the law
	1. Google Autonomous vehicle test case
5. Oversight and transparency
6. Access and distributive justice
7. Higher access to transportation (given short term car rentals or making cars a public good)
8. Saves lives
9. Blame and responsibility for accidents
10. Trust in the public eye (Too much trust or too little trust in self driving cars)
11. Geotracking and privacy
	1. Tesla currently collects recordings of drivers while on self-driving mode

## Trolley Optimism
Trolley problems *are* useful cases to think through philosophical problems

There are two kinds of Trolley Optimists
#### Democratic
The way we determine the morally best outcome for trolley problems is through a democratic vote and aggregating the result

##### Argument from Moral Relativism
See [[Moral Realism & Antirealism]]

##### Argument from Legitimacy
See [[Rawls]]
1. The legitimate exercise of coercive authority requires democratic procedures
2. If 1,  then we should vote to decide moral issues
3. So, we should vote to decide moral issues, such as trolley problems
#### Philosophical 
Instead of focusing on what the people think, we ought to be asking experts in moral philosophy. We dont care about the popular answer, we care about the *right* answer.

### The Intuitive Case
1. Trolley cases are tools for determining what we should do!
2. Some accident scenarios are structurally similar to trolley cases
3. If 1 and 2, then we should use trolley cases to directly inform how AVs are programmed to behave in accident scenarios that look like trolley cases
4. So, we should use trolley cases to directly inform how AVs are programmed to respond in accident scenarios
### Negligence
1. If we have the power to program cars to manage accident scenarios in morally better or worse ways, then we have an obligation to do so.
2. We do have the power to program cars to manage accident scenarios in morally better or worse ways
3. Trolley cases are the best tools for determining how to do so.
4. So, we have an obligation to manage accident scenarios in morally better or worse ways.
5. If 4, then Trolley Optimism
6. So, Trolley Optimism
	Lin, 2017

## Trolley Pessimism
Trolley problems *are not* useful cases to think through philosophical problems

### Disanalogy Objection
Nyholm and Smids, 2016

### Technological Objections
[[Basl]] and Behrends, 2020

1. Questions about how to program AVs to behave in accident situations cant be answered independently of other questions about ethical AV behavior design
	There are a lot of non accident scenarios that may create conditions for trolley cases. (such as law following, efficiency, congestion, parking, weather, recognizing construction)
2. If 1, then Trolley Pessimism
3. Therefore, Trolley Pessimism
	A2: Divide and conquer strategy: Why not make a separate model for every day driving and another for accident scenarios?
		A2A2: Its incredibly hard for a car to determine when to use one algorithm over the other

#### Other Important Ethical Decisions
1. Regime: How much of the regime should be dedicated to accident scenarios?
2. Structure:
3. Verdict:
4. Behavior: Should AVs be programmed to conform to our judgements about trolley cases in accident scenarios?