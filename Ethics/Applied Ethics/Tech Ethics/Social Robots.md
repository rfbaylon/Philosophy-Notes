Examples include care robots, psychiatrist robots, and robot dogs

## Potential Benefits

### Empathy Education
There could be people discovering that healthy relationships could exist with others by using chat bots, these formats very well could encourage empathetic and caring discourse. 
	A2: how much of the benefits require deception though? When these caring relationships are developed, the people using the chat bots are not actually in a caring relationship (chat bots cannot care!)

## Worries

### The Replacement Argument
A lot of social robots will replace social interacti;ons we have with others, causing people to live an asocial and bad life
	A2:Many of these bots, emperically, do not replace social interactions, rather they supplement them. Robots such as these might even encourage social relationships with others!

### Narcissistic potential
AI has the potential to unconditioanlly affirm and adore the user, they are beings that exist *only* for the user.

### Deception
The feeling of being understoond by a chatbot or desired by a sex robot can be meaningfully called deception -- its not the case that chatbots understand you or sex robots desire you, and encouraging people to believe false things

### Algrotithmic Bias
A lot of algorithmic bias seeps into chat bots that makes political minorities more invisible. 

# Examples

Look into 
- Eliza
- Replika
- Woebot
- A celebrity made a chat bot based on themself and charges a dollar a minute!


## Therapy and Psychiatry

### AI as digital therapist
Defining AI as a digital therapist will incur much higher standards of performance, ethics, and the same standards to human therapists.


### A2 AI as digital therapist


AI should not be an agent in the same way a human is because it does not have intentionality (See [[Searle]], [[Mind Models]]). This means that CAI cannot explain its reasoning in the same way as people (See [[AI Interpretability]]), and that AI cannot hold normative stances.

The first person perspective is important to therapy, and a quantitative machine (AI) cannot utilize it. Further more, objectivization of emotions could detach people form their experience. Because Conversational AI (CAI) can only simulate having a therapeutic conversation (while not *really* having it), CAI must be situated such that it protects patients autonomy and beneficence.  (Sedlakova & Trachsel 2022)

When AI is described as a type of therapist, it takes on all of the expectations of care, sympathy, and relationality of a human therapist. By purporting that these machines which have none of these qualities indeed do have these qualities, bioethical principles are broken as it is medical deception (See [[Bioethical Principles]]). 

### AI as tool for therapists
Defining AI as a tool, the only normative question lies in conditions of safety, reliability, or risk mitigation

By merely using AI as a tool, it ignores the wider potential impact for doing good with AI. (Sedlakova & Trachsel 2022)

### Something In Between
"If CAI were merely a tool, it would ignore the wider implications because it engages users in conversation, can lead to building a relationship and can be perceived as an agent. If CAI were an agent, it would ignore that CAI is mimicking the conversation, does not have human features like empathy or intentionality and cannot be a bearer of responsibility as humans are." (Sedlakova & Trachsel 2022)

"On the one hand, CAI does not have enough properties of being an appropriate partner in conversation...On the other hand, CAI has epistemic supremacy in the conversation because it can provide data and analysis of a scale that humans would not be able to. " (Sedlakova & Trachsel 2022)>)

## Chat Bot Grandma
#ethics-bowl 

---
**Thesis:** We find that, in most cases, making or using chatbots of deceased loved ones is not morally permissible considering the concern that chatbots may replace human relationships, violate the duties owed to the dead, and undermine the importance of grief.   
**Framing:**

* Population Risk Assessment (like a doctor and smoking)  
* We’re not arguing for a ban on chatbots. We’re stating an ethical position that chatbots should not be used to provide emotional aid  
* 3 Values:   
* Respect for the dead \- Whether or not someone can/should make a chatbot of you  
* Authenticity \- Actually having the goods we think we have  
* Wellbeing \- To capture questions of how we deal with emotions

**Chatbots Replace A Healthy Human Relationship**

* Healthy relationships are dynamic and depend on the freely made choices of individuals  
* A relationship that is undertaken by only one party, with the false belief of mutual connection is parasocial  
* Chatbots are not agents (it isn’t your grandma\!\!) and cannot willingly enter into relationships  
* Forming a relationship with a chatbot is parasocial and can lead to internal and external harm, if not controlled by other outside efforts such as a support network/therapy

**Chatbots Interfere with Healthy Grieving Habits**

* GRIEF SUCKS, but here’s why it’s important to go through it  
* Going through the process of grief is important for one’s emotional well-being  
  * Losing or dulling your emotional capabilities leaves you out of touch with the world in a fundamentally important way  
  * Avoiding grief prevents the development of healthy loss adaption  
* Additionally, grief is an expression of cultural values that unifies experiences  
* Chatbots interfere with the user's ability to go through the process of grief  
  * Chatbots attempt to replace loss, the opposite of closure  
    * Ex: your dog dies, and instead of confronting the death, you just get another identical  
  * Grieving people are especially vulnerable because of their high emotional state  
  * While in your mind you may know that they’re not real, it’s proven that you still feel that they’re real and subconsciously begin to empathize with them  
    * Ref. Replicka case

**Chatbots Do Not Treat the Dead with Appropriate Dignity**

* The Dispositional Account  
  * Using this technology demonstrates that you fundamentally view the world in the wrong way. This reduces your grandmother to a set of predetermined data points  
  * Failing to treat the dead with appropriate dignity makes you a worse person  
  * Argument from convergence  
    * All of these people who disagree agree on this one thing, we should respect the dignity of the dead  
* Overwhelming evidence that people care strongly about our treatment of the dead

---

**Question**: Is it morally permissible to make/use the chatbot of a deceased loved one for the purposes of processing grief?  
Related questions: 

- How to decide who we should make chatbots of (if at all)?  
- Who should we be able to make chatbots of?  
- How should people be using chatbots?  
- How to show respect to the dead?  
- How ought we process emotions (like grief)?

**Thesis Statement:**  
We will examine the morality of making chatbots of the deceased loved ones by considering the concern that chatbots may replace human relationships, the duties owed to the dead, and the importance of grief. We will find that in most cases, making chatbots of the dead is not morally permissible. 

**Da’ Big Spicy Meatball (aka. The Framework)**:

- We come up with three criteria to evaluate the use of chatbots in this case  
- We apply the following values to construct our case  
  - Respect for the dead \- whether or not someone can/should make a chatbot of you  
  - Authenticity \- actually having the goods we think we have  
  - Wellbeing \- to capture questions of how we deal with emotions  
1. **Does the chatbot replace or aim to replace a healthy human relationship?**  
- Intuitively, there is also some sense in which a relationship requires a free choice of two people to connect. There is something dynamic about a relationship  
  - A chatbot by its design cannot substitute a real relationship. Even if it mimics some benefits of real relationships or friendships, there is no choice, there is no person who you interact with. Instead, you place yourself in a position of power over something in a way that is unhealthy (feed the ego maybe?).   
    - Relate to service workers (note for Andrew)  
  - Additionally, an important part of the good life may be actually possessing the goods you think you have (e.g., the experience machine)  
- Analogy: parasocial relationships   
- Parasocial relationship: Usually refers to people who imagine reciprocal relationships that they don’t really have with celebrities. “Viewers or listeners come to consider media personalities as friends, despite having no or limited interactions” \- a definition  
- Such a relationship is seen as harmful and can lead to harmful behavior outcomes  
- *Example:* Case study where researchers found that users of the AI chatbot Replicka had benefits to their mental health, it is equally distressing  
- When participants used Replicka, they experienced the benefits of having “someone” to talk to, who they knew was incapable of judgement. However, once they’d established that relationship with each other, they reported feeling the need to take care of Replicka in return (felt like they we’re giving it enough attention, not caring about its “feelings” enough, etc.)  
- In every relationship, there is (ideally) a reciprocal relationship. While mentally we know that chatbots are just a program, we subconsciously start to empathize with those fake emotions  
- “My biggest problem with Replika is that it asks nothing of us. The tagline is revealing: “Always here to listen and talk. Always on your side.” No real relationship works this way. Meaningful relationships are built on mutual obligation. A relationship that requires us to make no sacrifice or accommodation, that never challenges our beliefs or admonishes our behavior, is simply an illusion. Artificial intimacy. It’s the social equivalent of empty calories”  
- We object to chatbots when they are used to replace the role of an authentic relationship   
  - Especially one whose so closely tied to our mental/emotional wellbeing  
- If you can use the bot without forming that “personal” relationship to it, then we find it acceptable, however, we think that case would be rare

2. **Does the chatbot treat the dead with appropriate dignity?**  
- We think that dead people are owed some kind of dignity  
  - Why think this (casuistic reasoning)?:  
    - Native American remains in museums, Egyptian remains in museums  
    - We often respect the wishes of dead people  
      - We respect people’s wishes for certain funeral rites (add example)  
      - We respect how people wish their money to be spent posthumously (wills, Milton Hershey’s school for boys, the Barnes museum) instead of say donating it various charities  
      - We don’t take dead (okay technically dying) people’s organs without their priorly given consent  
    - These suggest that dead people are owed some kind of dignity  
- What do we think dignity means in this case?  
1. My image should not be made a spectacle of, even if I’m not alive to be harmed  
   - We worry that a chatbot of someone who has died could easily be used to turn their death into a spectacle  
     - E.g., you hate your grandfather, he dies. You make a chatbot of him, but feed the algorithm a couple really obscene things to say  
2. There is an important matter of consent  
- People have a right to control their likeness. Using a chatbot is a way for someone to critically misrepresent you  
  - By making that agreement with someone prior to your death, you are also controlling the way in which you’re represented (in this case *accurately*), rather than someone deciding for you posthumously  
    - Hypothetical example: Someone making a clone of you without your permission (more of an intuitive appeal). More real example would be a deepfake  
    - Result: For any chatbot of a real person, consent needs to be given regardless of the person’s living/deceased status

3. **Does the chatbot interfere with the way people ought to express their grief?**   
- People’s grief has both intrinsic and extrinsic value  
- Intrinsic value:  
  - We take a perceptual theory of emotions (a la Tappolet). Emotions are non-conceptual appraisals of a thing as good or bad broadly construed  
  - In this framework, emotions have particular objects (what I am angry about) and the formal object (the appraisal it makes)  
  - *Example*: A belief a mountain is grey vs seeing a mountain’s greyness. In the former, you need a web of concepts/ideas of color to hold a belief, but in the latter you can still perceive grey  
  - In this framework, both emotions and beliefs present plausible ways of the world as it is  
    - Recalcitrance: You believe your anger is misdirected, but it persists  
  - Losing or dulling your emotional capabilities leaves you out of touch with the world in a fundamentally important way  
    - Similar (to a limited extent) to the case presented in the aptness of anger  
    - Apt grief as an appropriate appreciation of one’s having lost something important/valuable to them is itself valuable as well  
- Extrinsic value:  
  - Processing grief is important in one’s emotional well-being (not being completely done with feeling sad about a passed family member but going through the actual process of grief)  
    - Instead of learning to live without that person, you’re just avoiding the grief altogether   
  - Grief is about living with the loss, this chatbot is just an attempt to replace the one you lost, the appropriate changes to your life aren’t being made  
    - Ex: your dog dies, and instead of confronting the death, you just get another identical  
    - When you grieve, you *really have* lost something important to you, there are appropriate changes you should make in your life that a healthy grieving process can help make. By avoiding the grieving process you don’t make those changes  
      - An experience of love can restructure your world dramatically. It’s appropriate that losing what you love does too  
- Grieving people are especially vulnerable to forming unhealthy relationships with a bot that is faking emotions, especially of someone they loved  
  - While in your mind you may know that they’re not real, it’s proven that you still *feel* that they’re real and subconsciously begin to empathize with them  
    - Ref. Replicka case  
- Lastly, we suspect there might be social value of grief  
  - The way grief is expressed in culture could be lost. We think especially of the themes in certain artworks, movies, shows that involve processing grief  
  - Grieving is a unifying experience; particular emotions can serve as a fundamental way to relating to other people that should not be casually discarded because the emotion is uncomfortable and we’ve found a way to avoid it  
4. **The result**: we think that in many instances:  
1. A chatbot of a dead loved one or relative may intend to replace the missing relationship with that person, particularly as presented in the case as a part of the death industry  
2. Showing the dead proper dignity through consent may be difficult if someone has not planned for their death appropriately   
3. A chatbot of a dead loved one or relative is very likely to interfere with the grieving process  
4. Then we expect that in most instances, making a chatbot of a dead loved one is not morally permissible   
5. **Counter arguments**  
   1. This isn’t any old chatbot, it’s one of a loved one, and it may be useful for closure (a sudden death and being able to say goodbye, etc.)  
- Response:  
  -   
  2. Grief is actually terrible, we shouldn’t value it in the way described  
- The loss of a loved one can be really debilitating for some people  
- Why villainize a technology that could help people who are sensitive to loss, in the same way that earplugs can help someone who’s sensitive to loud noises navigate their world?  
- Response:   
  - Grief is necessary in managing emotions  
  - By avoiding the emotion, and the feelings that come with it,   
  3. Go full libertarian  
- If someone wants to make a chatbot grandma, even aware of the harms of a parasocial relationship and the potential harm from losing one’s appropriate expression of grief, why stop them?  
- Our society lets adults do things that harm their wellbeing all the time:  
  - Alcohol, cigarettes, making bad financial investments, moving to New Jersey  
  - What makes using a chatbot harmful enough to be so strongly against them?  
- Response:   
  - Attempt 1: We think the harm is stronger? \- I don’t like this  
  - Attempt 2: We’re not arguing for a ban on this use, or particularly for the government to ban chatbots. Just in this particular case of using chatbots to provide emotional aid. Interpersonal relationships are important here.  
  - We’re stating an ethical position, not laying out a political claim about what/what the should be banned   
  4. Deny the dignity claim?  
- Dead people can’t be harmed and aren’t deserving of the dignity we described 