# Longtermism
See [[MacAskill]], [[Ord]]
See [[The Precipice]]
### Is extinction bad? If so, why?
[[Ord]] takes the radical position (from extension of [[Nagel]]'s moral philosophy from [[The Possibility of Altruism]]) that extinction would be bad not only for the 7 billion people alive today, but for the quadrillions of potential people to inhabit the universe in the future. By going extinct, we are essentially ending the lives of quadrillions making X-Risks the most important issue to Ord. Ord also presupposes the modernist notion of [['Progress']]

Basis of Longtermism:
[[Nagel]] was [[Ord]]'s PhD advisor, so he was heavily influenced by Nagel's passages on [[Prudence]]
	"It is the condition that a person be equally real at all stages of his life; specifically, the fact that a particular stage is *present* cannot be regarded as conferring on it any special status" [[The Possibility of Altruism]], 60
	"The present is just a time among others, and confers no special status on the circumstances which occupy it" [[The Possibility of Altruism]], 61

## Examples of X Risks

- NOTE: the calculations for risk / century is done by [[Ord]], I do not know the methodology or if these are reliable estimates. PROCEED WITH CAUTION

### Natural X-Risks
- Note: A shit ton of X-Risks have only been discovered in the past 50 years, we can assume a shit ton more will be discovered soon
- ***Total natural risk: ~1/10,000 chance of extinction / century***
##### >10km Asteroid
*1/1,000,000 chance of strike / century*
##### Super-volcano
~*1/10,000 chance of eruption / century*
##### Stellar Explosions
*<1/1,000,000,000 chance of explosion / century (only for next century; avg is 1/5,000,000)*
Theres not much we can do lol

### Anthropogenic X-Risks
- Anthropogenic risks are 100-1000x more likely than natural events 
- ***Total anthropogenic risk: 1/6 chance of extinction / century***
##### Nuclear War
*1/1,000 risk / century*
Nuclear winter could drop the average atmospheric temperature by 7 degrees Celsius, leading to mass human extinction
Nuclear war close calls on ([[The Precipice]], 96) That shit is scary bro.

##### Climate Change
*1/1,000 risk / century*

There is more global warming since 1980 than ever before
A near infinite water vapor feedback is possible
Warming oceans couls trigger deep-ocean methan to rise into teh atmosphere, creating lots of warming.
There is still a decent chance we increase avg temperature by 9 degrees celcius
	At 12 degrees, only 1/2 of all land is habitable
##### Pandemics
*1/30 risk / century*

More facorty farming, more globalization -> substantially higher risk of horrible pandemic
Well-meaning scientists have *already* made an easily transmissable disease with a 60% death rate.
Pretty much no international lab standards, lol
	People with global death-drives also exist that could engineer an extremely deadly pandemic
		E.g., Shinrikyo death cult in Japan
	Can also be used as global blackmail by terrorists
##### Misaligned Artificial Intelligence
*1/10 risk / century*
General AI has the potential to end every other X-risk
The *only* cause for worry is the [[AI Alignment Problem]]
	AI could also hypothetically lock us in an unrecoverable dystopia
BIG risk BIG reward,,, we must be careful
##### Unrecoverable Dystopia
3 Kinds of Unrecoverable Dystopia
	1. Undesired (everyone is so technologically impotent and spread out that nobody can do anything)
	2. Enforced (there is a longterm hierarchial regime that keeps people in chains)
	3. Desired (an anarcho-primitivist social push that leads us away from X-Risk prevention)
##### Nanotech
Grey-goo
## A2 Longtermsim

### Intractability Objection
How do we even know how to best help future generations? Newton would have no way to help me because he had no clue how different the world would be 400 years from now?

### Demandingness Objection
Given Longtermism, we have to do *everything* we can to stop extinction. It outweighs literally every other concern ever.

### The Repugnant Conclusion

# Neutralism

## Neutralism

## A2 Neutralism

### Neartermism is an Example of the Identified Lives Bias \
[[Identified vs Statistical Lives]]