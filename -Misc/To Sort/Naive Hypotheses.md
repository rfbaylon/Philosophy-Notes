https://www.lesswrong.com/posts/ubdp8qAL8Gfki2pYo/naive-hypotheses-on-ai-alignment

If operant conditioning is the only pathway to learn something, and it requires some sort of punishment and reward system, then does AI learn? If so, is it sentient? Arguably, yes. 

[[The Simulation Hypothesis]] cannot be true as we can conceive of making simulations in the future. Any reasonable creator would code it such that simulations within simulations would be impossible as it could cause an infinite regress of computational power. The fact that the hypothesis is conceivable refutes its plausibility.

It could be the case that we owe invasive species more than native species as we had a direct hand in bringing them to their current residence