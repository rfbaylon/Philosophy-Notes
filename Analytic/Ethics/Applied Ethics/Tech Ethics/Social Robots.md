Examples include care robots, psychiatrist robots, and robot dogs

## Potential Benefits

### Empathy Education
There could be people discovering that healthy relationships could exist with others by using chat bots, these formats very well could encourage empathetic and caring discourse. 
	A2: how much of the benefits require deception though? When these caring relationships are developed, the people using the chat bots are not actually in a caring relationship (chat bots cannot care!)

## Worries

### The Replacement Argument
A lot of social robots will replace social interacti;ons we have with others, causing people to live an asocial and bad life
	A2:Many of these bots, emperically, do not replace social interactions, rather they supplement them. Robots such as these might even encourage social relationships with others!

### Narcissistic potential
AI has the potential to unconditioanlly affirm and adore the user, they are beings that exist *only* for the user.

### Deception
The feeling of being understoond by a chatbot or desired by a sex robot can be meaningfully called deception -- its not the case that chatbots understand you or sex robots desire you, and encouraging people to believe false things

### Algrotithmic Bias
A lot of algorithmic bias seeps into chat bots that makes political minorities more invisible. 

# Examples

Look into 
- Eliza
- Replika
- Woebot
- A celebrity made a chat bot based on themself and charges a dollar a minute!


## Therapy and Psychiatry

### AI as digital therapist
Defining AI as a digital therapist will incur much higher standards of performance, ethics, and the same standards to human therapists.


### A2 AI as digital therapist


AI should not be an agent in the same way a human is because it does not have intentionality (See [[Searle]], [[Mind Models]]). This means that CAI cannot explain its reasoning in the same way as people (See [[AI Interpretability]]), and that AI cannot hold normative stances.

The first person perspective is important to therapy, and a quantitative machine (AI) cannot utilize it. Further more, objectivization of emotions could detach people form their experience. Because Conversational AI (CAI) can only simulate having a therapeutic conversation (while not *really* having it), CAI must be situated such that it protects patients autonomy and beneficence.  (Sedlakova & Trachsel 2022)

When AI is described as a type of therapist, it takes on all of the expectations of care, sympathy, and relationality of a human therapist. By purporting that these machines which have none of these qualities indeed do have these qualities, bioethical principles are broken as it is medical deception (See [[Bioethical Principles]]). 

### AI as tool for therapists
Defining AI as a tool, the only normative question lies in conditions of safety, reliability, or risk mitigation

By merely using AI as a tool, it ignores the wider potential impact for doing good with AI. (Sedlakova & Trachsel 2022)

### Something In Between
"If CAI were merely a tool, it would ignore the wider implications because it engages users in conversation, can lead to building a relationship and can be perceived as an agent. If CAI were an agent, it would ignore that CAI is mimicking the conversation, does not have human features like empathy or intentionality and cannot be a bearer of responsibility as humans are." (Sedlakova & Trachsel 2022)

"On the one hand, CAI does not have enough properties of being an appropriate partner in conversation...On the other hand, CAI has epistemic supremacy in the conversation because it can provide data and analysis of a scale that humans would not be able to. " (Sedlakova & Trachsel 2022)>)